"""
Analytics engine for aggregation queries.
Handles both quantitative (database counts) and behavioral (AI analysis) questions.
"""
import logging
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum
from typing import Optional

import config
from db import Database
from search.vector_store import VectorStore
from search.embeddings import ChatService
from search.intent_detection import extract_analytics_type, extract_trait_from_question
from search.entity_aliases import get_all_forms, get_canonical

logger = logging.getLogger(__name__)


class AnalyticsType(Enum):
    """Type of analytics query."""
    QUANTITATIVE = "quantitative"  # Database aggregation
    BEHAVIORAL = "behavioral"       # AI analysis of content


@dataclass
class AnalyticsResult:
    """Result of an analytics query."""
    answer: str
    analytics_type: AnalyticsType
    stats: list[dict]
    total_analyzed: int = 0


class AnalyticsEngine:
    """
    Analytics engine for answering aggregation and behavioral questions.

    Handles:
    - "Who talks most?" -> count messages per user
    - "Who mentioned X most?" -> count term occurrences
    - "Who is more angry?" -> AI analysis of message tone
    """

    def __init__(
        self,
        db: Database,
        vector_store: VectorStore,
        chat_service: ChatService = None
    ):
        self.db = db
        self.vector_store = vector_store
        self.chat_service = chat_service or ChatService()

    # === Quantitative Analytics ===

    async def get_top_speakers(
        self,
        limit: int = None,
        date_from: str = None,
        date_to: str = None
    ) -> list[dict]:
        """
        Who talks the most? Return user stats sorted by message count.

        Returns:
            List of dicts with user_id, display_name, message_count
        """
        limit = limit or config.ANALYTICS_TOP_LIMIT

        results = self.db.get_message_count_by_user(
            date_from=date_from,
            date_to=date_to,
            limit=limit
        )

        return [
            {
                "user_id": user_id,
                "display_name": display_name,
                "message_count": count,
                "rank": i + 1
            }
            for i, (user_id, display_name, count) in enumerate(results)
        ]

    async def get_mention_counts(
        self,
        term: str,
        limit: int = None
    ) -> list[dict]:
        """
        Who mentioned X the most? Automatically expands aliases.

        Args:
            term: The term to search for (can be any alias form)
            limit: Max results to return

        Returns:
            List of dicts with user_id, display_name, mention_count, canonical_term
        """
        limit = limit or config.ANALYTICS_TOP_LIMIT

        # Expand term to all alias forms
        all_forms = get_all_forms(term)
        canonical = get_canonical(term)

        logger.info(f"Analytics: searching term '{term}' expanded to: {all_forms}")

        # Search for all forms at once
        results = self.db.get_term_mention_counts_multi(all_forms, limit=limit)

        return [
            {
                "user_id": user_id,
                "display_name": display_name,
                "mention_count": count,
                "rank": i + 1,
                "canonical_term": canonical,
                "searched_forms": all_forms
            }
            for i, (user_id, display_name, count) in enumerate(results)
        ]

    async def get_user_stats(self, user_id: int) -> dict:
        """Get comprehensive stats for a specific user."""
        return self.db.get_user_message_stats(user_id)

    # === Behavioral Analytics ===

    def _get_trait_search_queries(self, trait: str) -> list[str]:
        """Get search queries for a behavioral trait."""
        trait_queries = {
            "angry": [
                "–∑–ª–∏–π", "–±—ñ—Å–∏—Ç—å", "–¥—Ä–∞—Ç—É—î", "–Ω–µ–Ω–∞–≤–∏–¥–∂—É", "–∑–∞–¥–æ–≤–±–∞–ª–∏",
                "–∑–ª–æ–π", "–±–µ—Å–∏—Ç", "–¥–æ—Å—Ç–∞–ª–∏", "–Ω–µ–Ω–∞–≤–∏–∂—É", "–∑–∞–µ–±–∞–ª–∏"
            ],
            "strict": [
                "–º—É—Å–∏—à", "–ø–æ–≤–∏–Ω–µ–Ω", "–æ–±–æ–≤'—è–∑–∫–æ–≤–æ", "–∑–∞–±–æ—Ä–æ–Ω–µ–Ω–æ",
                "–¥–æ–ª–∂–µ–Ω", "–Ω–µ–ª—å–∑—è", "–æ–±—è–∑–∞–Ω", "–∑–∞–ø—Ä–µ—â–µ–Ω–æ"
            ],
            "psycho": [
                "–±–æ–∂–µ–≤—ñ–ª—å–Ω–∏–π", "—à–∏–∑–æ", "–ø–∞—Ä–∞–Ω–æ—è", "–ø—Å–∏—Ö–æ–∑",
                "—Å—É–º–∞—Å—à–µ–¥—à–∏–π", "—à–∏–∑–∞", "–ø–∞—Ä–∞–Ω–æ–∏–∫"
            ],
            "swears": [
                "–±–ª—è—Ç—å", "—Å—É–∫–∞", "—Ö—É–π", "–ø–∏–∑–¥–µ—Ü", "–Ω–∞—Ö—É–π",
                "–µ–±–∞—Ç—å", "–ø–∏–∑–¥–∞", "—Ö—É–π–Ω—è"
            ],
            "positive": [
                "—Å—É–ø–µ—Ä", "—á—É–¥–æ–≤–æ", "–º–æ–ª–æ–¥–µ—Ü—å", "–∫–ª–∞—Å–Ω–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ",
                "–æ—Ç–ª–∏—á–Ω–æ", "–∫—Ä—É—Ç–æ", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ", "—É—Ä–∞"
            ],
            "negative": [
                "–ø–æ–≥–∞–Ω–æ", "–∂–∞—Ö–ª–∏–≤–æ", "–∫–æ—à–º–∞—Ä", "–æ–≥–∏–¥–Ω–æ",
                "—É–∂–∞—Å–Ω–æ", "–ø–ª–æ—Ö–æ", "–¥–µ—Ä—å–º–æ", "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ"
            ],
            "kind": [
                "–¥—è–∫—É—é", "–≤–¥—è—á–Ω–∏–π", "–¥–æ–ø–æ–º–æ–≥—Ç–∏", "–ø—ñ–¥—Ç—Ä–∏–º—É—é",
                "—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—é", "–ø–æ–º–æ—á—å", "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é"
            ],
            "aggressive": [
                "–π–¥–∏ –Ω–∞—Ö—É–π", "–∑–∞—Ç–∫–Ω–∏—Å—å", "—ñ–¥—ñ–æ—Ç", "–¥–µ–±—ñ–ª",
                "–∏–¥–∏ –Ω–∞—Ö—É–π", "–∑–∞—Ç–∫–Ω–∏—Å—å", "–∏–¥–∏–æ—Ç", "–¥–µ–±–∏–ª"
            ],
            "toxic": [
                "—Ç–∏ —Ç—É–ø–∏–π", "–≤–∏ –≤—Å—ñ", "–Ω—ñ—Ö—Ç–æ –Ω–µ —Ä–æ–∑—É–º—ñ—î", "–≤—Å—ñ –¥—É—Ä–Ω—ñ",
                "—Ç—ã —Ç—É–ø–æ–π", "–≤—ã –≤—Å–µ", "–Ω–∏–∫—Ç–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç", "–≤—Å–µ –¥—É—Ä–∞–∫–∏"
            ],
        }
        return trait_queries.get(trait, [trait])

    async def _score_trait(self, trait: str, messages: list[str]) -> float:
        """
        Use AI to score how much messages exhibit a trait (0-10).

        Args:
            trait: The trait name
            messages: Sample messages to analyze

        Returns:
            Score from 0-10
        """
        if not messages:
            return 0.0

        # Take up to 10 messages for analysis
        sample = messages[:10]
        messages_text = "\n".join(f"- {m[:200]}" for m in sample)

        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —Ü—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∏, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É "{trait}" –∑–∞ —à–∫–∞–ª–æ—é 0-10.
0 = –∑–æ–≤—Å—ñ–º –Ω–µ –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—Ç—å
10 = –¥—É–∂–µ —Å–∏–ª—å–Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—Ç—å

–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:
{messages_text}

–í—ñ–¥–ø–æ–≤—ñ–¥—å –¥–∞–π –õ–ò–®–ï —á–∏—Å–ª–æ–º –≤—ñ–¥ 0 –¥–æ 10, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω—å."""

        try:
            response = await self.chat_service.complete_async(
                prompt=prompt,
                system="You are a text analyzer. Respond only with a single number 0-10.",
                max_tokens=10
            )
            # Parse the score
            score_str = response.strip().split()[0]
            score = float(score_str)
            return min(max(score, 0.0), 10.0)  # Clamp to 0-10
        except Exception as e:
            logger.warning(f"Trait scoring failed for {trait}: {e}")
            return 0.0

    async def analyze_behavioral_trait(
        self,
        trait: str,
        limit: int = 5
    ) -> list[dict]:
        """
        Find users who exhibit a behavioral trait most.

        1. Search for messages matching trait keywords
        2. Group by user
        3. Use AI to score trait intensity per user
        4. Return ranked list

        Args:
            trait: The trait to analyze (e.g., "angry", "strict")
            limit: Number of top users to return

        Returns:
            List of dicts with user_id, display_name, score, example_count
        """
        # Step 1: Search for trait-related messages
        trait_queries = self._get_trait_search_queries(trait)

        all_results = []
        seen_ids = set()

        for query in trait_queries:
            try:
                results = self.vector_store.search(
                    query=query,
                    n_results=50
                )
                for r in results:
                    vec_id = r.get("vector_id", "")
                    if vec_id not in seen_ids:
                        seen_ids.add(vec_id)
                        all_results.append(r)
            except Exception as e:
                logger.warning(f"Search failed for trait query '{query}': {e}")

        if not all_results:
            return []

        # Step 2: Group by user
        user_messages = defaultdict(list)
        user_display_names = {}

        for r in all_results:
            meta = r.get("metadata", {})
            user_id = meta.get("user_id")
            if user_id:
                user_messages[user_id].append(r.get("text", ""))
                if user_id not in user_display_names:
                    user_display_names[user_id] = meta.get("display_name", f"User#{user_id}")

        # Step 3: AI scoring for each user (limit to users with enough messages)
        user_scores = []
        for user_id, messages in user_messages.items():
            if len(messages) < 2:  # Need at least 2 messages for meaningful analysis
                continue

            score = await self._score_trait(trait, messages)
            user_scores.append({
                "user_id": user_id,
                "display_name": user_display_names.get(user_id, f"User#{user_id}"),
                "score": score,
                "example_count": len(messages)
            })

        # Step 4: Sort and return top users
        user_scores.sort(key=lambda x: x["score"], reverse=True)
        return user_scores[:limit]

    # === Main Entry Point ===

    async def answer_analytics_question(
        self,
        question: str,
        search_term: str = None
    ) -> AnalyticsResult:
        """
        Route analytics question to appropriate handler.

        Args:
            question: The user's question
            search_term: Optional extracted search term

        Returns:
            AnalyticsResult with answer text and stats
        """
        question_lower = question.lower()

        # Detect analytics type
        analytics_type_str = extract_analytics_type(question)
        analytics_type = (
            AnalyticsType.BEHAVIORAL if analytics_type_str == "behavioral"
            else AnalyticsType.QUANTITATIVE
        )

        if analytics_type == AnalyticsType.QUANTITATIVE:
            # Route to count-based analytics
            if any(kw in question_lower for kw in [
                '–Ω–∞–π–∞–∫—Ç–∏–≤–Ω—ñ—à', '–±—ñ–ª—å—à–µ –ø–∏—à–µ', '—Å–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π',
                '–±–æ–ª—å—à–µ –ø–∏—à–µ—Ç', 'most active', 'talks more', 'writes more'
            ]):
                stats = await self.get_top_speakers()
                answer = self._format_top_speakers(stats)
                return AnalyticsResult(
                    answer=answer,
                    analytics_type=analytics_type,
                    stats=stats,
                    total_analyzed=sum(s["message_count"] for s in stats)
                )

            elif any(kw in question_lower for kw in [
                '–∑–≥–∞–¥—É–≤–∞–≤', '—É–ø–æ–º–∏–Ω–∞–ª', 'mentioned', '–ø–∏—Å–∞–≤ –ø—Ä–æ', '–ø–∏—Å–∞–ª –ø—Ä–æ'
            ]):
                # Extract term from search_term or question
                term = search_term or self._extract_search_term(question)
                if term:
                    stats = await self.get_mention_counts(term)
                    answer = self._format_mention_stats(term, stats)
                    return AnalyticsResult(
                        answer=answer,
                        analytics_type=analytics_type,
                        stats=stats,
                        total_analyzed=sum(s["mention_count"] for s in stats)
                    )
                else:
                    return AnalyticsResult(
                        answer="–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —â–æ —Å–∞–º–µ —à—É–∫–∞—Ç–∏. –í–∫–∞–∂—ñ—Ç—å —Ç–µ—Ä–º—ñ–Ω –¥–ª—è –ø–æ—à—É–∫—É.",
                        analytics_type=analytics_type,
                        stats=[]
                    )

            else:
                # Default to top speakers
                stats = await self.get_top_speakers()
                answer = self._format_top_speakers(stats)
                return AnalyticsResult(
                    answer=answer,
                    analytics_type=analytics_type,
                    stats=stats,
                    total_analyzed=sum(s["message_count"] for s in stats)
                )

        else:
            # Behavioral analysis
            trait = extract_trait_from_question(question)
            if not trait:
                # Try to extract from the question directly
                trait = self._extract_trait_fallback(question)

            if trait:
                stats = await self.analyze_behavioral_trait(trait)
                answer = self._format_behavioral_stats(trait, stats)
                return AnalyticsResult(
                    answer=answer,
                    analytics_type=analytics_type,
                    stats=stats,
                    total_analyzed=sum(s["example_count"] for s in stats)
                )
            else:
                return AnalyticsResult(
                    answer="–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.",
                    analytics_type=analytics_type,
                    stats=[]
                )

    def _extract_search_term(self, question: str) -> Optional[str]:
        """Extract search term from mention-count questions."""
        import re

        # Patterns like "—Ö—Ç–æ –∑–≥–∞–¥—É–≤–∞–≤ X", "—Ö—Ç–æ –ø–∏—Å–∞–≤ –ø—Ä–æ X"
        patterns = [
            r'–∑–≥–∞–¥—É–≤–∞–≤\s+["\']?(.+?)["\']?(?:\s|$|,|\?)',
            r'—É–ø–æ–º–∏–Ω–∞–ª\s+["\']?(.+?)["\']?(?:\s|$|,|\?)',
            r'mentioned\s+["\']?(.+?)["\']?(?:\s|$|,|\?)',
            r'–ø–∏—Å–∞–≤\s+–ø—Ä–æ\s+["\']?(.+?)["\']?(?:\s|$|,|\?)',
            r'–ø–∏—Å–∞–ª\s+–ø—Ä–æ\s+["\']?(.+?)["\']?(?:\s|$|,|\?)',
            r'wrote\s+about\s+["\']?(.+?)["\']?(?:\s|$|,|\?)',
        ]

        for pattern in patterns:
            match = re.search(pattern, question.lower())
            if match:
                term = match.group(1).strip()
                if term and len(term) > 1:
                    return term

        return None

    def _extract_trait_fallback(self, question: str) -> Optional[str]:
        """Fallback trait extraction from question keywords."""
        question_lower = question.lower()

        # Direct keyword mapping
        fallback_traits = {
            '–∑–ª–∏–π': 'angry',
            '–∑–ª–æ–π': 'angry',
            '—Å—Ç—Ä–æ–≥–∏–π': 'strict',
            '–ø–æ–∑–∏—Ç–∏–≤–Ω': 'positive',
            '–Ω–µ–≥–∞—Ç–∏–≤–Ω': 'negative',
            '–ª–∞—î—Ç—å—Å—è': 'swears',
            '–º–∞—Ç—é–∫–∞—î—Ç—å—Å—è': 'swears',
            '—Ä—É–≥–∞–µ—Ç—Å—è': 'swears',
            '–¥–æ–±—Ä–∏–π': 'kind',
            '–¥–æ–±—Ä—ã–π': 'kind',
            '–∞–≥—Ä–µ—Å–∏–≤–Ω': 'aggressive',
            '—Ç–æ–∫—Å–∏—á–Ω': 'toxic',
        }

        for keyword, trait in fallback_traits.items():
            if keyword in question_lower:
                return trait

        return None

    def _format_top_speakers(self, stats: list[dict]) -> str:
        """Format top speakers for display."""
        if not stats:
            return "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É."

        lines = ["üèÜ –ù–∞–π–∞–∫—Ç–∏–≤–Ω—ñ—à—ñ —É—á–∞—Å–Ω–∏–∫–∏:", ""]
        medals = ["ü•á", "ü•à", "ü•â"]

        for i, stat in enumerate(stats[:10]):
            medal = medals[i] if i < 3 else f"{i + 1}."
            lines.append(
                f"{medal} üë§ {stat['display_name']} ‚Äî {stat['message_count']:,} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"
            )

        return "\n".join(lines)

    def _format_mention_stats(self, term: str, stats: list[dict]) -> str:
        """Format mention counts for display."""
        if not stats:
            return f"–ù—ñ—Ö—Ç–æ –Ω–µ –∑–≥–∞–¥—É–≤–∞–≤ ¬´{term}¬ª."

        lines = [f"üìä –•—Ç–æ –∑–≥–∞–¥—É–≤–∞–≤ ¬´{term}¬ª:", ""]

        for i, stat in enumerate(stats[:10]):
            rank = i + 1
            count = stat['mention_count']
            suffix = self._pluralize_times(count)
            lines.append(
                f"{rank}. üë§ {stat['display_name']} ‚Äî {count} {suffix}"
            )

        return "\n".join(lines)

    def _format_behavioral_stats(self, trait: str, stats: list[dict]) -> str:
        """Format behavioral analysis for display."""
        if not stats:
            return f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É ¬´{trait}¬ª."

        # Translate trait name
        trait_names = {
            'angry': '–∑–ª–∏–π/–∞–≥—Ä–µ—Å–∏–≤–Ω–∏–π',
            'strict': '—Å—Ç—Ä–æ–≥–∏–π',
            'positive': '–ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π',
            'negative': '–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π',
            'psycho': '–±–æ–∂–µ–≤—ñ–ª—å–Ω–∏–π',
            'swears': '–ª–∞—î—Ç—å—Å—è',
            'kind': '–¥–æ–±—Ä–∏–π',
            'aggressive': '–∞–≥—Ä–µ—Å–∏–≤–Ω–∏–π',
            'toxic': '—Ç–æ–∫—Å–∏—á–Ω–∏–π',
        }
        trait_display = trait_names.get(trait, trait)

        lines = [f"üîç –•—Ç–æ –Ω–∞–π–±—ñ–ª—å—à ¬´{trait_display}¬ª:", ""]

        for i, stat in enumerate(stats[:5]):
            rank = i + 1
            score = stat['score']
            count = stat['example_count']
            lines.append(
                f"{rank}. üë§ {stat['display_name']} ‚Äî –æ—Ü—ñ–Ω–∫–∞ {score:.1f}/10 "
                f"(–∑–Ω–∞–π–¥–µ–Ω–æ {count} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å)"
            )

        return "\n".join(lines)

    def _pluralize_times(self, count: int) -> str:
        """Pluralize '—Ä–∞–∑/—Ä–∞–∑–∏/—Ä–∞–∑—ñ–≤' in Ukrainian."""
        if count % 10 == 1 and count % 100 != 11:
            return "—Ä–∞–∑"
        elif 2 <= count % 10 <= 4 and (count % 100 < 10 or count % 100 >= 20):
            return "—Ä–∞–∑–∏"
        else:
            return "—Ä–∞–∑—ñ–≤"
